{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goals: Feature Engineering\n",
        "\n",
        "This notebook merge _Brazil_ and _France_ dataset into a single training dataset.\n",
        "It adds seasonal information (seasons, month), scale relevent features and removed unecessary columns.\n",
        "\n",
        "> Note this notebook need ouputs from both _01a - Data Preprocessing Brazil_ and 01b - _Data Preprocessing France_\n",
        "\n",
        "![Alt text](../images/notebook-2.png)\n",
        "\n",
        "### 1. Data Import and Setup\n",
        "\n",
        "Imports necessary libraries, sets up environment paths, and includes custom utility functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
        "sys.path.append(BASE_DIR)\n",
        "\n",
        "from src.utils.plots import plot_water_flows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defines constants :\n",
        "\n",
        "- INPUT_DIR must be the same as the one defined in _01 - Data Preprocessing_ notebook.\n",
        "- EVAL_DIR must be the same as the one defined in _01 - Data Preprocessing_ notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIR = \"../../../data/input/\"\n",
        "EVAL_DIR = \"../../../data/evaluation/\"\n",
        "\n",
        "datasets = {\"train\": INPUT_DIR, \"eval\": EVAL_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Loading and Initial Cleaning\n",
        "\n",
        "- Reads in the French and Brazilian baseline datasets, removes unnecessary columns, and sets the date as the index.\n",
        "- Introduces a binary indicator (`north_hemisphere`) to distinguish between data from France and Brazil.\n",
        "- Merges the French and Brazilian datasets into a single DataFrame for further processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_baseline = {}\n",
        "\n",
        "for set, dir in datasets.items():\n",
        "    path_data_baseline_france = f\"{dir}preprocessed_france.csv\"\n",
        "    dataset_baseline_france = pd.read_csv(path_data_baseline_france)\n",
        "\n",
        "    dataset_baseline_france = dataset_baseline_france.iloc[:, 1:]\n",
        "    dataset_baseline_france = dataset_baseline_france.drop(columns=[\"index\"])\n",
        "\n",
        "    dataset_baseline_france = dataset_baseline_france.set_index(\"ObsDate\")\n",
        "\n",
        "    path_data_baseline = f\"{dir}preprocessed_brazil.csv\"\n",
        "    dataset_baseline_brazil = pd.read_csv(path_data_baseline)\n",
        "\n",
        "    # remove the first column\n",
        "    dataset_baseline_brazil = dataset_baseline_brazil.iloc[:, 1:]\n",
        "    dataset_baseline_brazil = dataset_baseline_brazil.drop(columns=[\"index\"])\n",
        "\n",
        "    dataset_baseline_brazil = dataset_baseline_brazil.set_index(\"ObsDate\")\n",
        "\n",
        "    dataset_baseline_france[\"north_hemisphere\"] = 1\n",
        "    dataset_baseline_brazil[\"north_hemisphere\"] = 0\n",
        "\n",
        "    dataset_baseline[set] = pd.concat(\n",
        "        [dataset_baseline_france, dataset_baseline_brazil], axis=0\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Feature Engineering\n",
        "\n",
        "Creates seasonal and monthly indicator columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for set, dir in datasets.items():\n",
        "    # Convert the index to datetime and extract the month\n",
        "    month = pd.to_datetime(dataset_baseline[set].index).month\n",
        "\n",
        "    # Define season mappings\n",
        "    seasons = {\n",
        "        \"is_winter\": [1, 2, 3],\n",
        "        \"is_spring\": [4, 5, 6],\n",
        "        \"is_summer\": [7, 8, 9],\n",
        "        \"is_autumn\": [10, 11, 12],\n",
        "    }\n",
        "\n",
        "    # Apply season flags\n",
        "    for season, months in seasons.items():\n",
        "        dataset_baseline[set][season] = month.isin(months)\n",
        "\n",
        "    # Define month abbreviations and apply monthly flags\n",
        "    months_abbr = [\n",
        "        \"jan\",\n",
        "        \"feb\",\n",
        "        \"mar\",\n",
        "        \"apr\",\n",
        "        \"may\",\n",
        "        \"jun\",\n",
        "        \"jul\",\n",
        "        \"aug\",\n",
        "        \"sep\",\n",
        "        \"oct\",\n",
        "        \"nov\",\n",
        "        \"dec\",\n",
        "    ]\n",
        "    for i, abbr in enumerate(months_abbr, start=1):\n",
        "        dataset_baseline[set][f\"is_{abbr}\"] = month == i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applies MinMax scaling to the selected features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for set, dir in datasets.items():\n",
        "    scaler = MinMaxScaler()\n",
        "    cols = dataset_baseline[set].columns\n",
        "    # remove the water_flows columns\n",
        "    if set == \"eval\":\n",
        "        cols = cols.drop([\"station_code\", \"water_flow_lag_1w\", \"water_flow_lag_2w\"])\n",
        "    elif set == \"train\":\n",
        "        cols = cols.drop(\n",
        "            [\n",
        "                \"water_flow_week1\",\n",
        "                \"station_code\",\n",
        "                \"water_flow_week2\",\n",
        "                \"water_flow_week3\",\n",
        "                \"water_flow_week4\",\n",
        "                \"water_flow_lag_1w\",\n",
        "            ]\n",
        "        )\n",
        "    dataset_baseline[set][cols] = scaler.fit_transform(dataset_baseline[set][cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Handling Missing Data\n",
        "\n",
        "Removes undesired columns, identifies columns with missing values, and imputes them with their respective column means.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove columns that start with index_\n",
        "for set, dir in datasets.items():\n",
        "    cols = dataset_baseline[set].columns\n",
        "    cols = cols[~cols.str.startswith(\"index_\")]\n",
        "    dataset_baseline[set] = dataset_baseline[set][cols]\n",
        "\n",
        "    # find columns that contains nan values\n",
        "\n",
        "    cols_nan = (\n",
        "        dataset_baseline[set].columns[dataset_baseline[set].isna().any()].tolist()\n",
        "    )\n",
        "\n",
        "    # impute nan values with the mean\n",
        "    for col in cols_nan:\n",
        "        dataset_baseline[set][col] = dataset_baseline[set][col].fillna(\n",
        "            dataset_baseline[set][col].mean()\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Data Saving, and Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saves the complete baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for set, dir in datasets.items():\n",
        "    dataset_baseline[set].to_csv(f\"{dir}dataset_baseline.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizes the water flow for the 10 first stations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_water_flows(dataset_baseline[\"train\"], max_stations=2, display=True, save=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
