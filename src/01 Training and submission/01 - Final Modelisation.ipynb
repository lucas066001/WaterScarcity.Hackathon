{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Training the *Final* Models\n",
    "\n",
    "This notebook trains the model on the full *baseline_dataset* for the final prediction on evaluation data.\n",
    "\n",
    "Here, we train a model designed to generalize across water stations in Brazil and France. However, you are not required to follow this approach and may opt to train separate models for different geographic *regions*.\n",
    "\n",
    "This baseline model training example utilizes all available features, with hyperparameters chosen for quick execution rather than optimization. For hyperparameter tuning and feature selection explorations, refer to the `02_exploration` folder.\n",
    "\n",
    "> **Note:** This notebook requires outputs from the `00 Preprocessing` notebooks.\n",
    "\n",
    "<img src=\"../images/notebook-3.png\" alt=\"Experiment Diagram\" style=\"width:75%; text-align:center;\" />\n",
    "\n",
    "### 1. Data Import and Setup\n",
    "\n",
    "This section imports the necessary libraries, sets up environment paths, and includes custom utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from mapie.regression import MapieQuantileRegressor\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..')))\n",
    "\n",
    "from src.utils.model import split_dataset, compare_models_per_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constants :\n",
    "- **INPUT_DIR**: Directory for input data (same as in \"02 - Feature Engineering\").\n",
    "- **MODEL_DIR**: Directory where trained models are saved.\n",
    "- **DATASET_DIR**: Directory where the Zenodo dataset is unzipped.\n",
    "\n",
    "##### Model Parameters\n",
    "\n",
    "- **SEED**: 42 (for reproducibility)\n",
    "- **NUMBER_OF_WEEK**: 4 (one model is trained per week)\n",
    "\n",
    "##### FINAL_MODELS\n",
    "\n",
    "- **mapie**: Combines LightGBM with MAPIE. **MAPIE** (Model Agnostic Prediction Interval Estimator) computes prediction intervals for any regression model using conformal methods.\n",
    "- **qrf**: Quantile Random Forest (natively produces prediction intervals)\n",
    "- **ebm**: Explainable Boosting Machine is used as a exemple that does not natively implement prediction intervals, but that can be customised to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../data/input/\"\n",
    "MODEL_DIR = \"../../../models/\"\n",
    "DATASET_DIR = \"../../../dataset/\"\n",
    "\n",
    "SEED = 42\n",
    "NUMBER_OF_WEEK = 4 # Number of weeks to predict one model is trained per week\n",
    "\n",
    "FINAL_MODELS = [\"mapie\",\n",
    "                \"qrf\",\n",
    "                #\"EBM\"\n",
    "                ]\n",
    "mapie_enbpi = {}\n",
    "mapie = {}\n",
    "qrf = {}\n",
    "mapie_aci = {}\n",
    "\n",
    "COLUMNS_TO_DROP = [\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading\n",
    "Load in the baseline datasets, create the directory to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(f\"{INPUT_DIR}dataset_baseline.csv\")\n",
    "\n",
    "dataset_train = dataset_train.set_index(\"ObsDate\")\n",
    "\n",
    "if not os.path.exists(f\"{MODEL_DIR}final/\"):\n",
    "    os.makedirs(f\"{MODEL_DIR}final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing removal of unnecessary columns, setup of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset_train.drop(columns=COLUMNS_TO_DROP)\n",
    "y_train = {}\n",
    "for i in range(0, NUMBER_OF_WEEK):\n",
    "    y_train[i] = dataset_train[f\"water_flow_week{i+1}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models training\n",
    "#### a. LGBM + MAPIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapie Model Training Overview\n",
    "\n",
    "- **Configuration:**  \n",
    "  - Sets `ALPHA` (0.1) as the prediction interval level.\n",
    "  - Defines `TIME_VALIDATION` as a split point for creating a validation set.\n",
    "  - Configures LightGBM parameters (`LGBM_PARAMS`) for quantile regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "TIME_VALIDATION = \"2000-01-01 00:00:00\"\n",
    "\n",
    "LGBM_PARAMS = {\n",
    "    \"max_depth\": 15,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\": 500,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"objective\": \"quantile\",\n",
    "    \"alpha\": ALPHA\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Preparation:**  \n",
    "  - Splits `dataset_train` into training and validation subsets using `split_dataset`.\n",
    "  - Removes unnecessary columns from both the training and validation datasets.\n",
    "  - Extracts target variables for each week (from `water_flow_week1` to `water_flow_week4`).\n",
    "\n",
    "- **Model Training:**  \n",
    "  For each week:\n",
    "  - Initializes a LightGBM regressor with the specified parameters.\n",
    "  - Wraps it in a `MapieQuantileRegressor` to estimate prediction intervals.\n",
    "  - Trains the model on the training data and calibrates it using the validation data.\n",
    "  - Saves the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mapie\" in FINAL_MODELS: \n",
    "    print(\"Training Mapie\")\n",
    "\n",
    "\n",
    "    train_mapie, val_mapie, val_temporal  = split_dataset(dataset_train, 0.75, TIME_VALIDATION)\n",
    "\n",
    "    X_train_mapie = train_mapie.drop(columns=COLUMNS_TO_DROP)\n",
    "    print(len(X_train_mapie.columns))\n",
    "    y_train_mapie = {}\n",
    "    for i in range(0, NUMBER_OF_WEEK):\n",
    "        y_train_mapie[i] = train_mapie[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "    X_val = val_mapie.drop(columns=COLUMNS_TO_DROP)\n",
    "    y_val = {}\n",
    "    y_val[0] = val_mapie[\"water_flow_week1\"]\n",
    "    for i in range(1, NUMBER_OF_WEEK):\n",
    "        y_val[i] = val_mapie[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training week {i}\")\n",
    "        # Initialize and train MapieQuantileRegressor\n",
    "        regressor = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "        mapie[i] = MapieQuantileRegressor(estimator=regressor, method=\"quantile\", cv=\"split\", alpha=ALPHA)\n",
    "        mapie[i].fit(X_train_mapie, y_train_mapie[i], X_calib=X_val, y_calib=y_val[i])\n",
    "        \n",
    "        # save model with date\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        model_path = f\"{MODEL_DIR}final/mapie_quantile_{time}_week_{i}.pkl\"\n",
    "        joblib.dump(mapie[i], model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. QRF\n",
    "\n",
    "- **Training:**  \n",
    "  Initializes a `RandomForestQuantileRegressor` with the following parameters:\n",
    "  - 100 estimators\n",
    "  - Maximum depth of 10\n",
    "  - Minimum of 10 samples per leaf\n",
    "\n",
    "  These parameters allow for relatively fast training, though they are not optimized for peak performance. \n",
    "  \n",
    "  The model is then fitted using `X_train` and the corresponding weekly target `y_train[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"qrf\" in FINAL_MODELS:\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training week {i}\")\n",
    "        # Train RandomForestQuantileRegressor\n",
    "        qrf[i] = RandomForestQuantileRegressor(n_estimators=100, max_depth=5, min_samples_leaf=5)\n",
    "        qrf[i].fit(X_train, y_train[i])\n",
    "\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        model_path = f\"{MODEL_DIR}final/qrf_quantile_{time}_week_{i}.pkl\"\n",
    "        joblib.dump(qrf[i], model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Explainable Boosting Machine\n",
    "\n",
    "EBM is an ensemble method that does not natively provide access to its individual members for performing quantile predictions or generating prediction intervals. To overcome this limitation, we manually construct an ensemble.\n",
    "\n",
    "\n",
    "- **Ensemble Training:**  \n",
    "- For each ensemble member (seed from 0 to 4):\n",
    "    - A bootstrap sample is created from `X_train` and `y_train[i]` using sampling with replacement.\n",
    "    - An `ExplainableBoostingRegressor` is instantiated with fixed parameters (e.g., `max_bins=128`, `learning_rate=0.05`, `interactions=3`, and `random_state=42` to ensure consistent binning) and then trained on the sampled data.\n",
    "    - The trained model is appended to the list for the current week.\n",
    "- **Saving the Ensemble:**  \n",
    "- The ensemble (i.e., the list of EBM models for the week) is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ebm\" in FINAL_MODELS:\n",
    "    NUM_ENSEMBLES = 5\n",
    "    ebm_ensembles = {}\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training EBM ensemble for week {i}\")\n",
    "\n",
    "        models_i = []\n",
    "        for seed in range(NUM_ENSEMBLES):\n",
    "            print(f\"Training EBM ensemble {seed} for week {i}\")\n",
    "            # 1. Create your bootstrap sample or subset (if you want bagging)\n",
    "            sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "            X_sample = X_train.iloc[sample_indices]\n",
    "            y_sample = y_train[i][sample_indices]\n",
    "            \n",
    "            # 2. Train an EBM with consistent binning parameters\n",
    "            ebm_model = ExplainableBoostingRegressor(\n",
    "                outer_bags=1,\n",
    "                inner_bags=1,\n",
    "                max_bins=128,\n",
    "                learning_rate=0.05,\n",
    "                interactions=3,\n",
    "                early_stopping_rounds=100,\n",
    "                random_state=SEED\n",
    "            )\n",
    "            ebm_model.fit(X_sample, y_sample)\n",
    "            \n",
    "            models_i.append(ebm_model)\n",
    "\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        file_path = f\"{MODEL_DIR}final/ebm_ensemble_{time}_week_{i}.pkl\"\n",
    "\n",
    "        joblib.dump(ebm_ensembles, file_path)\n",
    "        print(f\"Saved EBM ensembles to {file_path}\")\n",
    "\n",
    "        # Store the list of models for week i\n",
    "        ebm_ensembles[i] = models_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Performance Evaluation on the Full Training Set\n",
    "\n",
    "> **Note:**  \n",
    "> The performance displayed here is calculated on the training set. This does not necessarily reflect the models' performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_stations = dataset_train[\"station_code\"].values\n",
    "\n",
    "for i in range(NUMBER_OF_WEEK):\n",
    "    predictions = []\n",
    "    baseline_day_before = dataset_train[\"water_flow_lag_1w\"]\n",
    "    predictions.append({\"model\": \"Week before\", \"prediction\": baseline_day_before, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "    if \"mapie\" in FINAL_MODELS:\n",
    "        y_pred_mapie, y_pis_mapie = mapie[i].predict(X_train)\n",
    "        predictions.append({\"model\": \"LGBM+MAPIE\", \"prediction\": y_pred_mapie, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_mapie})\n",
    "    if \"qrf\" in FINAL_MODELS:\n",
    "        y_pred_qrf = qrf[i].predict(X_train, quantiles=\"mean\", aggregate_leaves_first=False)\n",
    "        y_pis_qrf = qrf[i].predict(X_train, quantiles=[ALPHA/2, 1-ALPHA/2])\n",
    "        predictions.append({\"model\": \"QRF\", \"prediction\": y_pred_qrf, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_qrf})\n",
    "    if \"ebm\" in FINAL_MODELS:\n",
    "        y_pred_ebm = []\n",
    "        for model in ebm_ensembles[i]:\n",
    "            y_pred_ebm.append(model.predict(X_train))\n",
    "        y_pred_ebm = np.mean(y_pred_ebm, axis=0)\n",
    "        predictions.append({\"model\": \"EBM\", \"prediction\": y_pred_ebm, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "\n",
    "    compare_models_per_station(\n",
    "        y_train[i].values,\n",
    "        predictions,\n",
    "        y_train_stations,\n",
    "        column_to_display=\"log_likelihood\" ,\n",
    "        title = f\"WEEK {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Coverage on the Full Training Set\n",
    "\n",
    "> **Note:**  \n",
    "> The performance displayed here is calculated on the training set. This does not necessarily reflect the models' performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUMBER_OF_WEEK):\n",
    "\n",
    "    predictions = []\n",
    "    baseline_day_before = dataset_train[\"water_flow_lag_1w\"]\n",
    "    predictions.append({\"model\": \"Week before\", \"prediction\": baseline_day_before, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "    if \"mapie\" in FINAL_MODELS:\n",
    "        y_pred_mapie, y_pis_mapie = mapie[i].predict(X_train)\n",
    "        predictions.append({\"model\": \"LGBM+MAPIE\", \"prediction\": y_pred_mapie, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_mapie})\n",
    "        coverage = (y_train[i].values >= y_pis_mapie[:,0,0]) & (y_train[i].values <= y_pis_mapie[:,1,0])\n",
    "        print(f\"MAPIE coverage of the prediction interval for week {i}: {coverage.mean()}\")\n",
    "    if \"qrf\" in FINAL_MODELS:\n",
    "        y_pred_qrf = qrf[i].predict(X_train, quantiles=\"mean\", aggregate_leaves_first=False)\n",
    "        y_pis_qrf = qrf[i].predict(X_train, quantiles=[ALPHA/2, 1-ALPHA/2])\n",
    "        predictions.append({\"model\": \"QRF\", \"prediction\": y_pred_qrf, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_qrf})\n",
    "        coverage = (y_train[i].values >= y_pis_qrf[:,0]) & (y_train[i].values <= y_pis_qrf[:,1])\n",
    "        print(f\"QRF coverage of the prediction interval for week {i}: {coverage.mean()}\")\n",
    "    if \"ebm\" in FINAL_MODELS:\n",
    "        y_pred_ebm = []\n",
    "        for model in ebm_ensembles[i]:\n",
    "            y_pred_ebm.append(model.predict(X_train))\n",
    "        y_pred_ebm = np.mean(y_pred_ebm, axis=0)\n",
    "        predictions.append({\"model\": \"EBM\", \"prediction\": y_pred_ebm, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "        coverage = (y_train[i].values >= y_pis_qrf[:,0]) & (y_train[i].values <= y_pis_qrf[:,1])\n",
    "        print(f\"EBM coverage of the prediction interval for week {i}: {coverage.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
