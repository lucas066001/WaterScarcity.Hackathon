{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Feature Engineering\n",
    "\n",
    "This notebook merge *Brazil* and *France* dataset into a single training dataset.\n",
    "It adds seasonal information (seasons, month), scale relevent features and removed unecessary columns.\n",
    "\n",
    "> Note this notebook need ouputs from both *01a - Data Preprocessing Brazil* and 01b - *Data Preprocessing France*\n",
    "\n",
    "![Alt text](../images/notebook-2.png)\n",
    "\n",
    "### 1. Data Import and Setup\n",
    "\n",
    "Imports necessary libraries, sets up environment paths, and includes custom utility functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..'))\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "from src.utils.plots import plot_water_flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines constants :\n",
    "* INPUT_DIR must be the same as the one defined in *01 - Data Preprocessing* notebook.\n",
    "* EVAL_DIR must be the same as the one defined in *01 - Data Preprocessing* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../data/input/\"\n",
    "EVAL_DIR = \"../../../data/evaluation/\"\n",
    "\n",
    "datasets = { \"train\": INPUT_DIR, \"eval\" : EVAL_DIR}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Initial Cleaning\n",
    "\n",
    "* Reads in the French and Brazilian baseline datasets, removes unnecessary columns, and sets the date as the index.\n",
    "* Introduces a binary indicator (`north_hemisphere`) to distinguish between data from France and Brazil.\n",
    "* Merges the French and Brazilian datasets into a single DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_baseline = {}\n",
    "\n",
    "for set, dir in datasets.items():\n",
    "    path_data_baseline_france = f\"{dir}preprocessed_france.csv\"\n",
    "    dataset_baseline_france = pd.read_csv(path_data_baseline_france)\n",
    "\n",
    "    dataset_baseline_france = dataset_baseline_france.iloc[:,1:]\n",
    "    dataset_baseline_france = dataset_baseline_france.drop(columns=[\"index\"])\n",
    "\n",
    "\n",
    "    dataset_baseline_france = (\n",
    "        dataset_baseline_france\n",
    "        .set_index(\"ObsDate\")\n",
    "    )\n",
    "\n",
    "    path_data_baseline = f\"{dir}preprocessed_brazil.csv\"\n",
    "    dataset_baseline_brazil = pd.read_csv(path_data_baseline)\n",
    "\n",
    "    # remove the first column\n",
    "    dataset_baseline_brazil = dataset_baseline_brazil.iloc[:,1:]\n",
    "    dataset_baseline_brazil = dataset_baseline_brazil.drop(columns=[\"index\"])\n",
    "\n",
    "    dataset_baseline_brazil = (\n",
    "        dataset_baseline_brazil\n",
    "        .set_index(\"ObsDate\")\n",
    "    )\n",
    "\n",
    "\n",
    "    dataset_baseline_france[\"north_hemisphere\"] = 1\n",
    "    dataset_baseline_brazil[\"north_hemisphere\"] = 0\n",
    "\n",
    "    dataset_baseline[set] = pd.concat([dataset_baseline_france, dataset_baseline_brazil], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering\n",
    "\n",
    "Creates seasonal and monthly indicator columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set, dir in datasets.items():\n",
    "    # Convert the index to datetime and extract the month\n",
    "    month = pd.to_datetime(dataset_baseline[set].index).month\n",
    "\n",
    "    # Define season mappings\n",
    "    seasons = {\n",
    "        \"is_winter\": [1, 2, 3],\n",
    "        \"is_spring\": [4, 5, 6],\n",
    "        \"is_summer\": [7, 8, 9],\n",
    "        \"is_autumn\": [10, 11, 12],\n",
    "    }\n",
    "\n",
    "    # Apply season flags\n",
    "    for season, months in seasons.items():\n",
    "        dataset_baseline[set][season] = month.isin(months)\n",
    "\n",
    "    # Define month abbreviations and apply monthly flags\n",
    "    months_abbr = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\",\n",
    "                \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "    for i, abbr in enumerate(months_abbr, start=1):\n",
    "        dataset_baseline[set][f\"is_{abbr}\"] = month == i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies MinMax scaling to the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set, dir in datasets.items():\n",
    "    scaler = MinMaxScaler()\n",
    "    cols = dataset_baseline[set].columns\n",
    "    # remove the water_flows columns\n",
    "    if set == \"eval\":\n",
    "        cols = cols.drop([\"station_code\", \"water_flow_lag_1w\", \"water_flow_lag_2w\"])\n",
    "    elif set == \"train\":\n",
    "        cols = cols.drop([\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\", \"water_flow_lag_1w\"])\n",
    "    dataset_baseline[set][cols] = scaler.fit_transform(dataset_baseline[set][cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handling Missing Data\n",
    "\n",
    "Removes undesired columns, identifies columns with missing values, and imputes them with their respective column means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that start with index_\n",
    "for set, dir in datasets.items():\n",
    "    cols = dataset_baseline[set].columns\n",
    "    cols = cols[~cols.str.startswith(\"index_\")]\n",
    "    dataset_baseline[set] = dataset_baseline[set][cols]\n",
    "\n",
    "    # find columns that contains nan values\n",
    "\n",
    "    cols_nan = dataset_baseline[set].columns[dataset_baseline[set].isna().any()].tolist()\n",
    "\n",
    "    # impute nan values with the mean\n",
    "    for col in cols_nan:\n",
    "        dataset_baseline[set][col] = dataset_baseline[set][col].fillna(dataset_baseline[set][col].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Saving, and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the complete baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set, dir in datasets.items():\n",
    "    dataset_baseline[set].to_csv(f\"{dir}dataset_baseline.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizes the water flow for the 10 first stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_water_flows(dataset_baseline[\"train\"], max_stations = 2, display = True, save = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
