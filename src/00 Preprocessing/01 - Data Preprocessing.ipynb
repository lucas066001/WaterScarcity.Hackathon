{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Preprocessing of training Data and Inference Data\n",
    "\n",
    "Preprocessing, alignement and aggregation, into a single usable csv file, of : \n",
    "* Meteo data\n",
    "* Waterflow discharge \n",
    "* DEM\n",
    "* Soil composition\n",
    "* Station position\n",
    "\n",
    "Using informations in Hydrographic Areas.\n",
    "\n",
    "![Alt text](../images/notebook-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n",
    "Imports necessary libraries, sets up environment paths, and includes custom utility functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict\n",
    "import rioxarray\n",
    "\n",
    "# Third-party library imports\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rioxarray.exceptions import NoDataInBounds \n",
    "from shapely.geometry import Point, box\n",
    "import xarray as xr\n",
    "\n",
    "# Modify sys.path to include custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..')))\n",
    "\n",
    "from src.utils.preprocessing import prepare_kdtree, interpolate_variable, interpolate_and_merge_optimized, get_altitude\n",
    "from src.utils.plots import plot_hydrographic_maps\n",
    "from src.utils.data_loader import load_hydro_data, read_soil_data, read_altitude_data, load_water_flows, load_station_info, load_meteo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines constants :\n",
    "* *DATASET_DIR* must be the directory where you unzip the *zenodo* dataset.\n",
    "* *EVAL_DIR* will be used to store inference / evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBOX = {\n",
    "    \"brazil\": [-44.5, -21.5, -39.5, -17.5],\n",
    "    \"france\": [-3, 42.0, 8, 49.0]\n",
    "}\n",
    "\n",
    "AREAS = [\"brazil\", \"france\"]\n",
    "DATASET_DIR = \"../../../dataset/\"\n",
    "EVAL_DIR = \"../../../data/evaluation/\"\n",
    "INPUT_DIR = \"../../../data/input/\"\n",
    "\n",
    "datasets = { \"train\": INPUT_DIR, \"eval\" : EVAL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Initial Cleaning\n",
    "* Hydrological divisions\n",
    "* Climate data (precipitation, soil moisture, evaporation and temperature)\n",
    "* Station metadata for localisation of water station\n",
    "* Water flow discharge\n",
    "* Digital elevation model\n",
    "* And soil data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_region = {}\n",
    "gdf_sector = {}\n",
    "gdf_sub_sector = {}\n",
    "gdf_zone = {}\n",
    "precipitations = {}\n",
    "temperatures = {}\n",
    "soil_moisture = {}\n",
    "evaporation = {}\n",
    "stations = {}\n",
    "water_flows = {}\n",
    "soil_ds = {}\n",
    "\n",
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "\n",
    "    # 1. Load hydrographic data\n",
    "    hydro_data = load_hydro_data(area, DATASET_DIR)\n",
    "    gdf_region[area] = hydro_data['region']\n",
    "    gdf_sector[area] = hydro_data['sector']\n",
    "    gdf_sub_sector[area] = hydro_data['sub_sector']\n",
    "    gdf_zone[area] = hydro_data['zone']\n",
    "\n",
    "    # 2. Load soil composition data\n",
    "    soil_ds[area] = read_soil_data(area, DATASET_DIR)\n",
    "\n",
    "    # 3. Load altitude data\n",
    "    dem = read_altitude_data(area, DATASET_DIR)\n",
    "    \n",
    "    for type, dataset in datasets.items():\n",
    "        print(\"Processing\", type)\n",
    "\n",
    "        key = f\"{type}_{area}\"\n",
    "\n",
    "        # 4. Loading meteo data\n",
    "        meteo = load_meteo_data(area, type, DATASET_DIR)\n",
    "        precipitations[key] = meteo[\"precipitations\"]\n",
    "        temperatures[key]   = meteo[\"temperatures\"]\n",
    "        soil_moisture[key]  = meteo[\"soil_moisture\"]\n",
    "        evaporation[key]    = meteo[\"evaporation\"]\n",
    "        \n",
    "        # 5. Load station info\n",
    "        stations[key] = load_station_info(area, type, DATASET_DIR)\n",
    "        stations[key]['altitude'] = stations[key].apply(lambda x: get_altitude(x['latitude'], x['longitude'], dem), axis=1)\n",
    "\n",
    "        # 6. Load water flows\n",
    "        water_flows[key] = load_water_flows(area, type, DATASET_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding rows for predictions within water_flows this is specific to the inference / evaluation data pre-processing the new line correspond to the first week to be infered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "    key = f\"eval_{area}\"\n",
    "    new_dfs = []\n",
    "    block_size = 4\n",
    "\n",
    "    for i in range(0, len(water_flows[key]), block_size):\n",
    "        block = water_flows[key].iloc[i : i + block_size].copy()\n",
    "        new_dfs.append(block)\n",
    "        \n",
    "        if len(block) == block_size:\n",
    "            last_row = block.iloc[-1].copy()\n",
    "            last_row['ObsDate'] = last_row['ObsDate'] + pd.Timedelta(days=7)\n",
    "            last_row['discharge'] = float('nan')\n",
    "            new_dfs.append(pd.DataFrame([last_row]))\n",
    "\n",
    "    water_flows[key] = pd.concat(new_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hydrographic Area Association \n",
    "Adding corresponding Hydrological divisions (*'regions'*, *'secteurs'*, *'sous-secteurs'*, and *'zones'* - and equivalent for Brazil) for every station.\n",
    "\n",
    "> All the stations have discharge information and will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "    for type, dataset in datasets.items():\n",
    "        print(\"Processing\", type)\n",
    "        key = f\"{type}_{area}\"\n",
    "\n",
    "        stations[key]['geometry'] = stations[key].apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "        gdf_stations = gpd.GeoDataFrame(stations[key], geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "        for hydro in (gdf_region, gdf_sector, gdf_sub_sector, gdf_zone):\n",
    "            hydro[area] = hydro[area].to_crs(gdf_stations.crs)\n",
    "\n",
    "        # Set join parameters based on the area\n",
    "        if area == \"france\":\n",
    "            join_info = {\n",
    "                \"region\":    (\"CdRegionHydro\",      \"_stations\",      \"_region\"),\n",
    "                \"sector\":    (\"CdSecteurHydro\",     \"_stations\",      \"_sector\"),\n",
    "                \"sub_sector\":(\"CdSousSecteurHydro\", \"_stations\",      \"_sub_sector\"),\n",
    "                \"zone\":      (\"CdZoneHydro\",        \"_stations\",      \"_zone\")\n",
    "            }\n",
    "        elif area == \"brazil\":\n",
    "            join_info = {\n",
    "                \"region\":    (\"wts_pk\", \"_stations_region\", \"_region\"),\n",
    "                \"sector\":    (\"wts_pk\", \"_stations_sector\", \"_sector\"),\n",
    "                \"sub_sector\":(\"wts_pk\", \"_stations_sub_sector\", \"_sub_sector\"),\n",
    "                \"zone\":      (\"wts_pk\", \"_stations_zone\", \"_zone\")\n",
    "            }\n",
    "        else:\n",
    "            continue  # Skip unsupported areas\n",
    "\n",
    "        # Map layer names to their corresponding GeoDataFrames\n",
    "        hydro_dict = {\n",
    "            \"region\":     gdf_region[area],\n",
    "            \"sector\":     gdf_sector[area],\n",
    "            \"sub_sector\": gdf_sub_sector[area],\n",
    "            \"zone\":       gdf_zone[area]\n",
    "        }\n",
    "\n",
    "        # Perform spatial joins for each hydrographic layer in a loop\n",
    "        for level, (col, lsuffix, rsuffix) in join_info.items():\n",
    "            gdf_stations = gpd.sjoin(\n",
    "                gdf_stations,\n",
    "                hydro_dict[level][['geometry', col]],\n",
    "                how=\"left\",\n",
    "                predicate=\"within\",\n",
    "                lsuffix=lsuffix,\n",
    "                rsuffix=rsuffix\n",
    "            )\n",
    "\n",
    "        # Drop the geometry column if it's no longer needed\n",
    "        stations[key] = gdf_stations.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filter and Prepare Spatial Data\n",
    "\n",
    "Filter hydrological divisions to keep only relevant (*'regions'*, *'secteurs'*, *'sous-secteurs'*, and *'zones'* - and equivalent for Brazil) based on associated station data.\n",
    "\n",
    "> We remove hydrological divisions where there are no stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_list = {}\n",
    "for area in AREAS:\n",
    "    # eval have all used stations train just have a subset of used stations\n",
    "    key = f\"eval_{area}\"\n",
    "\n",
    "    print(\"Processing\", area)\n",
    "    gdf_region[area] = gdf_region[area][gdf_region[area].index.isin(stations[key]['index__region'].unique())]\n",
    "    gdf_sector[area] = gdf_sector[area][gdf_sector[area].index.isin(stations[key]['index__sector'].unique())]\n",
    "    gdf_sub_sector[area] = gdf_sub_sector[area][gdf_sub_sector[area].index.isin(stations[key]['index__sub_sector'].unique())]\n",
    "    gdf_zone[area] = gdf_zone[area][gdf_zone[area].index.isin(stations[key]['index__zone'].unique())]\n",
    "    gdf_region[area].name = \"index__region\"\n",
    "    gdf_sector[area].name = \"index__sector\"\n",
    "    gdf_sub_sector[area].name = \"index__sub_sector\"\n",
    "    gdf_zone[area].name = \"index__zone\"\n",
    "    gdf_list[area] = [gdf_region[area], gdf_sector[area], gdf_sub_sector[area], gdf_zone[area]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    plot_hydrographic_maps(area, {\n",
    "        \"region\": gdf_region,\n",
    "        \"sector\": gdf_sector,\n",
    "        \"sub_sector\": gdf_sub_sector,\n",
    "        \"zone\": gdf_zone\n",
    "    }, BBOX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Soil Data aggregation\n",
    "Mean estimation of soil composition : \n",
    "* Bulk density\n",
    "* Coarse fragments volumetric fraction\n",
    "* Clay particles \n",
    "* Sand particles\n",
    "\n",
    "Aggregated at the 4 hydrological division levels and 6 depth intervals.\n",
    "> This step can take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "    name_soil_var = []\n",
    "    soil_ds[area] = soil_ds[area].rename({var: var.replace(f\"{area}_\", \"\") for var in soil_ds[area].data_vars})\n",
    "    for gdf in gdf_list[area]: # 4 hydrological divisions\n",
    "        print(gdf.name)\n",
    "        for soil_data in soil_ds[area].data_vars:\n",
    "            name_soil_var.append(f\"{soil_data}_{gdf.name}\")\n",
    "            for idx, row in gdf.iterrows():\n",
    "                region_geom = [row.geometry].copy()\n",
    "                try:\n",
    "                    clipped_bdod = soil_ds[area][soil_data].rio.clip(region_geom, gdf.crs)\n",
    "                    mean_val = float(clipped_bdod.mean().values)\n",
    "                except NoDataInBounds:\n",
    "                    mean_val = np.nan\n",
    "                    print(f\"No data in bounds for {soil_data}_{gdf.name}\")\n",
    "                gdf.at[idx, f\"{soil_data}_{gdf.name}\"] = mean_val\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare Climate and Water Flow Data\n",
    "\n",
    "Convert meteo variables \n",
    "* Temperature \n",
    "* Precipitation\n",
    "* Soil moisture\n",
    "* Evaporation\n",
    "\n",
    "to DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperatures = {}\n",
    "df_precipitations = {}\n",
    "df_soil_moisture = {}\n",
    "df_evaporation = {}\n",
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "\n",
    "    for type, dataset in datasets.items():\n",
    "        key = f\"{type}_{area}\"\n",
    "        print(\"Processing\", type)\n",
    "        \n",
    "        for source, col, dest in [\n",
    "            (temperatures, \"t2m\", df_temperatures),\n",
    "            (precipitations, \"tp\", df_precipitations),\n",
    "            (soil_moisture, \"swvl1\", df_soil_moisture),\n",
    "            (evaporation, \"e\", df_evaporation)\n",
    "        ]:\n",
    "            dest[key] = (\n",
    "                source[key]\n",
    "                .to_dataframe()\n",
    "                .reset_index()\n",
    "                .filter(items=[\"valid_time\", \"latitude\", \"longitude\", col])\n",
    "                .rename(columns={\"valid_time\": \"ObsDate\"})\n",
    "                .fillna(0)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Merge water flow dataset with station \n",
    "\n",
    "Merge water_flows dataset with station to retrieve stations informations (latitude, longitude, altitude, hydrographic divisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:    \n",
    "    print(\"Processing\", area)\n",
    "    for type in datasets.keys():\n",
    "        key = f\"{type}_{area}\"\n",
    "        print(\"Processing\", type)\n",
    "\n",
    "        water_flows[key] = water_flows[key].merge(\n",
    "            stations[key],\n",
    "            on=\"station_code\",\n",
    "            how=\"left\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clean and Align Water Flow Data\n",
    "\n",
    "Filter water flow data to match the specified time period. Identify and remove stations with invalid (negative) water flow values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "    for type in datasets.keys():\n",
    "        key = f\"{type}_{area}\"\n",
    "        print(\"Processing\", type)\n",
    "\n",
    "        water_flows[key] = water_flows[key][[\"discharge\",\n",
    "                                                \"latitude\",\n",
    "                                                \"longitude\",\n",
    "                                                \"catchment\",\n",
    "                                                \"altitude\",\n",
    "                                                \"index__sector\",\n",
    "                                                \"index__sub_sector\",\n",
    "                                                \"index__zone\",\n",
    "                                                \"index__region\",\n",
    "                                                \"station_code\",\n",
    "                                                \"ObsDate\"]].copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Spatial Interpolation and aggregation of the Meteo data\n",
    "\n",
    "Perform interpolation and merge at the 4 hydrological division levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "    for type in datasets.keys():\n",
    "        key = f\"{type}_{area}\"\n",
    "        print(\"Processing\", type)\n",
    "\n",
    "        kdtree = prepare_kdtree(\n",
    "            precipitations[key].latitude.values,\n",
    "            precipitations[key].longitude.values\n",
    "        )\n",
    "\n",
    "        # Interpolate each meteo variable and store the result in water_flows\n",
    "        measurement_data = {\n",
    "            \"precipitations\": precipitations[key].tp,\n",
    "            \"temperatures\": temperatures[key].t2m,\n",
    "            \"soil_moisture\": soil_moisture[key].swvl1,\n",
    "            \"evaporation\": evaporation[key].e,\n",
    "        }\n",
    "        for meteo_key, data in measurement_data.items():\n",
    "            water_flows[key][meteo_key] = interpolate_variable(water_flows[key], kdtree, data)\n",
    "\n",
    "        # Define spatial groups with their corresponding GeoDataFrame and index column name\n",
    "        groups = [\n",
    "            (\"region\", gdf_region[area], \"index__region\"),\n",
    "            (\"zone\", gdf_zone[area], \"index__zone\"),\n",
    "            (\"sector\", gdf_sector[area], \"index__sector\"),\n",
    "            (\"sub_sector\", gdf_sub_sector[area], \"index__sub_sector\"),\n",
    "        ]\n",
    "\n",
    "        # Define measurements for merging with the appropriate singular names\n",
    "        merge_measurements = {\n",
    "            \"precipitation\": precipitations[key].tp,\n",
    "            \"temperature\": temperatures[key].t2m,\n",
    "            \"soil_moisture\": soil_moisture[key].swvl1,\n",
    "            \"evaporation\": evaporation[key].e,\n",
    "        }\n",
    "\n",
    "        # Process all combinations of measurements and spatial groups\n",
    "        for group_name, gdf, idx_col in groups:\n",
    "            for meas, data in merge_measurements.items():\n",
    "                col_name = f\"{meas}_{group_name}\"\n",
    "                water_flows[key] = interpolate_and_merge_optimized(\n",
    "                    water_flows[key], kdtree, data, gdf, col_name, idx_col\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Merge Soil Data with Water Flow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "    for type in datasets.keys():\n",
    "        key = f\"{type}_{area}\"\n",
    "        print(\"Processing\", type)\n",
    "\n",
    "        for gdf in gdf_list[area]:\n",
    "            cols = [col for col in gdf.columns if col in name_soil_var]\n",
    "            water_flows[key] = water_flows[key].merge(gdf[cols], left_on=gdf.name, how=\"left\", right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Create Lagged Water Flow and Meteo Features\n",
    "\n",
    "Create lagged features for one and two weeks to capture temporal dependencies, and for meteo variable (Temperatures, Precipitations and Evaporations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_baseline = {}\n",
    "\n",
    "for key in water_flows.keys():\n",
    "    print(\"Processing\", key)\n",
    "\n",
    "    water_flows[key] = water_flows[key].reset_index()\n",
    "    water_flows[key]['ObsDate'] = pd.to_datetime(water_flows[key]['ObsDate'])\n",
    "    df = water_flows[key].copy()\n",
    "\n",
    "    dataset_baseline[key] = (\n",
    "        df\n",
    "        .groupby(['station_code', pd.Grouper(key='ObsDate', freq='W')])\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "    dataset_baseline[key]['water_flow_lag_1w'] = (\n",
    "        dataset_baseline[key]\n",
    "        .groupby('station_code')['discharge']\n",
    "        .shift(1)\n",
    "    )\n",
    "\n",
    "    dataset_baseline[key]['water_flow_lag_2w'] = (\n",
    "        dataset_baseline[key]\n",
    "        .groupby('station_code')['discharge']\n",
    "        .shift(2)\n",
    "    )\n",
    "\n",
    "    # Create lagged precipitation, temperature, evaporation features (previous values for -1 week)\n",
    "    for col in dataset_baseline[key].columns:\n",
    "        if col.startswith(\"precipitation\") or col.startswith(\"temperature\") or col.startswith(\"evaporation\"):\n",
    "            dataset_baseline[key][f\"{col}_lag_1w\"] = (\n",
    "                dataset_baseline[key]\n",
    "                .groupby('station_code')[col]\n",
    "                .shift(1)\n",
    "            )\n",
    "    \n",
    "    # If we are in the train dataset, we can create the target variables for the next 4 weeks\n",
    "    if key.startswith(\"train\"):\n",
    "        dataset_baseline[key]['water_flow_lag_2w'] = (\n",
    "            dataset_baseline[key]\n",
    "            .groupby('station_code')['discharge']\n",
    "            .shift(2)\n",
    "        )\n",
    "        dataset_baseline[key]['water_flow_week2'] = (\n",
    "            dataset_baseline[key]\n",
    "            .groupby('station_code')['discharge']\n",
    "            .shift(-1)\n",
    "        )\n",
    "        dataset_baseline[key]['water_flow_week3'] = (\n",
    "            dataset_baseline[key]\n",
    "            .groupby('station_code')['discharge']\n",
    "            .shift(-2)\n",
    "        )\n",
    "        dataset_baseline[key]['water_flow_week4'] = (\n",
    "            dataset_baseline[key]\n",
    "            .groupby('station_code')['discharge']\n",
    "            .shift(-3)\n",
    "        )\n",
    "\n",
    "        dataset_baseline[key] = dataset_baseline[key].rename(columns={\"discharge\": \"water_flow_week1\"})\n",
    "\n",
    "    dataset_baseline[key] = dataset_baseline[key].dropna(subset=['water_flow_lag_1w'])\n",
    "    dataset_baseline[key] = dataset_baseline[key].dropna(subset=['water_flow_lag_2w'])\n",
    "\n",
    "    # add CdRegionHydro, CdSecteurHydro, CdSousSecteurHydro, CdZoneHydro from water_flows\n",
    "\n",
    "    columns = ['station_code']\n",
    "\n",
    "    dataset_baseline[key] = dataset_baseline[key].merge(\n",
    "        water_flows[key][columns].drop_duplicates(),\n",
    "        on='station_code',\n",
    "        how='left'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Drop of rows where no predictions are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove the discharge column from the *eval* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "    key = f\"eval_{area}\"\n",
    "\n",
    "    # Only keep the inference rows\n",
    "    missing_values = dataset_baseline[key][\"discharge\"].isnull()\n",
    "    dataset_baseline[key] = dataset_baseline[key][missing_values]\n",
    "\n",
    "# drop discharge column\n",
    "for area in AREAS:\n",
    "    key = f\"eval_{area}\"\n",
    "    print(\"Processing\", area)\n",
    "    dataset_baseline[key] = dataset_baseline[key].drop(columns=[\"discharge\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Saving Pre-processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data\n",
    "for area in AREAS:\n",
    "    print(\"Processing\", area)\n",
    "    for type, path in datasets.items():\n",
    "        key = f\"{type}_{area}\"\n",
    "\n",
    "        path_dataset_baseline = f\"{path}/preprocessed_{area}.csv\"\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        dataset_baseline[key].to_csv(path_dataset_baseline)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
