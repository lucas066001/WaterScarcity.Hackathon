{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Hyper Parameter Optimisation of *QRF* model\n",
    "\n",
    "This notebook propose different methods of hyper parameter optimisation based on X-Validation :\n",
    "* Random Search\n",
    "* Genetic algorithm [Not yet included]\n",
    "\n",
    "# 1. Data Import and Setup\n",
    "\n",
    "Imports necessary libraries, sets up environment paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Third-party imports\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Append project root to sys.path for local imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', '..')))\n",
    "\n",
    "# Local application imports\n",
    "from src.utils.model import get_station_stats, custom_log_likelihood\n",
    "from src.utils.SpatioTemporalSplit import SpatioTemporalSplit\n",
    "from src.utils.custom_models import SnowIndexComputeTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines constants :\n",
    "* INPUT_DIR must be the same as the one defined in *00 Preprocessing/Feature Engineering*.\n",
    "* MODEL_DIR is the directory where the exploration models will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../../data/input/\"\n",
    "MODEL_DIR = \"../../../../models/exploration/\"\n",
    "\n",
    "SEED = 42 \n",
    "ALPHA = 0.1\n",
    "WEEK_TO_PREDICT=4 \n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"rm_wl\", # remove custom generated water_flow_lag 3w & 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"slct_ma\", # keep only specific mobile average 2w or/and 3w or/and 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"lag_slope\" # add an indicator that is calculated between water_flow_lag 1w and 2w \n",
    "]\n",
    "\n",
    "PCA_THRESHOLD = 0.98\n",
    "N_CLUSTER = 3\n",
    "\n",
    "DATASET_SPEC = \"_\".join(DATASET_TRANSFORMS)\n",
    "\n",
    "if \"pca\" in DATASET_TRANSFORMS:\n",
    "    DATASET_SPEC += f\"_pct_{PCA_THRESHOLD}\"\n",
    "\n",
    "if \"clust_index\" in DATASET_TRANSFORMS:\n",
    "    DATASET_SPEC += f\"_geocl_{N_CLUSTER}\"\n",
    "\n",
    "if \"clust_hydro\" in DATASET_TRANSFORMS:\n",
    "    DATASET_SPEC += f\"_hydcl_{N_CLUSTER}\"\n",
    "\n",
    "DATASET_SPEC = \"dataset_custom_11\"\n",
    "\n",
    "# columns to dropÂ : target at different horizon, station_code, and features removed from Feature Selection\n",
    "TO_DROP = [\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading\n",
    "Load in the baseline datasets, create the directory to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "ds_train = pd.read_csv(f\"{INPUT_DIR}{DATASET_SPEC}.csv\")\n",
    "train_data = ds_train.copy()\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data = train_data.loc[:, ~train_data.columns.duplicated()]\n",
    "ds_train = ds_train.set_index(\"ObsDate\")\n",
    "y_train = train_data[f\"water_flow_week{WEEK_TO_PREDICT}\"]\n",
    "cv_data = train_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model preparation\n",
    "\n",
    "Compute station statistics (usefull for scalling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_stats = get_station_stats(\n",
    "    y_train.to_numpy(),\n",
    "    train_data[\"station_code\"].to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom Pipeline to keep track of the station code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "cols_to_drop = TO_DROP.copy()\n",
    "cols_to_drop += [\"ObsDate\"]\n",
    "predictor_cols = [col for col in cv_data.columns if col not in cols_to_drop]\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('select', 'drop', cols_to_drop),\n",
    "    ('kbest_select', SelectKBest(score_func=f_regression, k=len(predictor_cols)), predictor_cols)\n",
    "], remainder='drop')\n",
    "snowIndexer = SnowIndexComputeTransformer()\n",
    "\n",
    "qrf_week1 = RandomForestQuantileRegressor(n_estimators=10, max_depth=10, min_samples_leaf=10)\n",
    "# qrf_week1 = GradientBoostingRegressor()\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    # ('snowindexer', SnowIndexComputeTransformer(temp_col_name=\"tempartures_pca_1\", rain_col_name=\"precipitations_pca_1\",)),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the log likelihood scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_log_likelihood(estimator, X, y_true, cv_data, station_stats, alpha=0.1):\n",
    "    return -custom_log_likelihood(estimator, X, y_true, cv_data, station_stats, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = partial(inverted_log_likelihood,\n",
    "                 cv_data=cv_data,\n",
    "                 station_stats=station_stats,\n",
    "                 alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the SpatioTemporal Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = SpatioTemporalSplit(\n",
    "    n_splits=10,\n",
    "    date_col='ObsDate',\n",
    "    station_col='station_code',\n",
    "    temporal_frac=0.68,\n",
    "    spatial_frac=0.68,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyper parameter tuning\n",
    "\n",
    "Define the hyperparameter distributions for random search, take care the parameters presented here are choosen so that the search is fast you need to explore wider parameters range.\n",
    "\n",
    "#### a. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chapu\\AppData\\Local\\Temp\\ipykernel_5168\\2609022489.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  cv_data[\"water_flow_evolve_slope\"].fillna(cv_data[\"water_flow_evolve_slope\"].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "cv_data[\"water_flow_evolve_slope\"].fillna(cv_data[\"water_flow_evolve_slope\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 45 candidates, totalling 450 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(12375, 0)) while a minimum of 1 is required by RandomForestQuantileRegressor.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 136, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\quantile_forest\\_quantile_forest.py\", line 170, in fit\n    super(BaseForestQuantileRegressor, self).fit(X, y, sample_weight=sample_weight)\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1096, in check_array\n    raise ValueError(\nValueError: Found array with 0 feature(s) (shape=(12375, 0)) while a minimum of 1 is required by RandomForestQuantileRegressor.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m      3\u001b[39m param_distributions = {\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# \"preprocessor__kbest_select__k\": np.arange(3, len(cv_data.columns) - 1, 3),\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel__n_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m5\u001b[39m, \u001b[32m15\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m55\u001b[39m, \u001b[32m85\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m130\u001b[39m, \u001b[32m150\u001b[39m, \u001b[32m190\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel__bootstrap\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[32m     11\u001b[39m }\n\u001b[32m     15\u001b[39m random_search = RandomizedSearchCV(\n\u001b[32m     16\u001b[39m     estimator=pipeline,\n\u001b[32m     17\u001b[39m     param_distributions=param_distributions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     error_score=\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m loc_params = {}\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# for region_index in range(N_CLUSTER):\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m#     clust_data = cv_data.copy(deep=True)\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#     clust_data = clust_data[clust_data[\"region_cluster\"] == region_index].copy(deep=True)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[38;5;66;03m#         \"nb_cols\": len(selected_mask[selected_mask == True])\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m#     }\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1013\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1014\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1015\u001b[39m     )\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1023\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1960\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1958\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1959\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1960\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1961\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1963\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1964\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    958\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    959\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    960\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    961\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    962\u001b[39m         )\n\u001b[32m    963\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    984\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    985\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    986\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    987\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    988\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     69\u001b[39m config = get_config()\n\u001b[32m     70\u001b[39m iterable_with_config = (\n\u001b[32m     71\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1748\u001b[39m \n\u001b[32m   1749\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1751\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1752\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1755\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    739\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    742\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    743\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    744\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chapu\\E.Lucas\\Perso\\Github\\WaterScarcity.Hackathon\\.venv\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 feature(s) (shape=(12375, 0)) while a minimum of 1 is required by RandomForestQuantileRegressor."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_distributions = {\n",
    "    \"preprocessor__kbest_select__k\": np.arange(3, len(cv_data.columns) - 1, 3),\n",
    "    'model__n_estimators': [5, 15, 35, 55, 85, 100, 130, 150, 190],\n",
    "    'model__max_depth': [2, 7, 13, 20, 30, 50, 70, 100],\n",
    "    'model__min_samples_leaf': [9, 15, 25, 35, 50, 60, 80, 105],\n",
    "    'model__min_samples_split': [9, 15, 25, 35, 50, 60, 80, 105],\n",
    "    'model__max_features': ['sqrt', 'log2', 0.3, 0.7, None],\n",
    "    'model__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "loc_params = {}\n",
    "\n",
    "for region_index in range(N_CLUSTER):\n",
    "    clust_data = cv_data.copy(deep=True)\n",
    "    clust_data = clust_data[clust_data[\"region_cluster\"] == region_index].copy(deep=True)\n",
    "    clust_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    y_train = clust_data[f\"water_flow_week{WEEK_TO_PREDICT}\"]\n",
    "\n",
    "    print(clust_data.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    station_stats = get_station_stats(\n",
    "        y_train.to_numpy(),\n",
    "        clust_data[\"station_code\"].to_numpy()\n",
    "    )\n",
    "\n",
    "    scorer = partial(inverted_log_likelihood,\n",
    "                     cv_data=clust_data,\n",
    "                     station_stats=station_stats,\n",
    "                     alpha=ALPHA)\n",
    "\n",
    "    cv_splitter = SpatioTemporalSplit(\n",
    "        n_splits=10,\n",
    "        date_col='ObsDate',\n",
    "        station_col='station_code',\n",
    "        temporal_frac=0.68,\n",
    "        spatial_frac=0.68,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    cv_indices = list(cv_splitter.split(clust_data, y_train))  # â la vraie correction ici\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=60,\n",
    "        scoring=scorer,\n",
    "        cv=cv_indices,\n",
    "        # random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=3,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    random_search.fit(clust_data, y_train)\n",
    "    \n",
    "    best_selector = random_search.best_estimator_.named_steps[\"preprocessor\"].named_transformers_[\"kbest_select\"]\n",
    "    selected_mask = best_selector.get_support()\n",
    "    print(len(selected_mask[selected_mask == True]))\n",
    "    adjusted_data = cv_data.drop(columns=cols_to_drop)\n",
    "    selected_features = adjusted_data.columns[selected_mask]\n",
    "\n",
    "    loc_params[region_index] = {\n",
    "        \"best_params_\": random_search.best_params_,\n",
    "        \"best_score_\": random_search.best_score_,\n",
    "        \"selected_features\": selected_features.tolist(),\n",
    "        \"nb_cols\": len(selected_mask[selected_mask == True])\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'best_params_': {'preprocessor__kbest_select__k': np.int64(3),\n",
       "   'model__n_estimators': 55,\n",
       "   'model__min_samples_split': 80,\n",
       "   'model__min_samples_leaf': 50,\n",
       "   'model__max_features': 'log2',\n",
       "   'model__max_depth': 50,\n",
       "   'model__bootstrap': False},\n",
       "  'best_score_': np.float64(-1.648296653845708),\n",
       "  'selected_features': ['catchment',\n",
       "   'water_flow_lag_1w',\n",
       "   'water_flow_ma_4w_lag_1w_gauss'],\n",
       "  'nb_cols': 3},\n",
       " 1: {'best_params_': {'preprocessor__kbest_select__k': np.int64(24),\n",
       "   'model__n_estimators': 35,\n",
       "   'model__min_samples_split': 60,\n",
       "   'model__min_samples_leaf': 9,\n",
       "   'model__max_features': None,\n",
       "   'model__max_depth': 13,\n",
       "   'model__bootstrap': True},\n",
       "  'best_score_': np.float64(-1.931898281421455),\n",
       "  'selected_features': ['index',\n",
       "   'precipitations_lag_1w_pca_2',\n",
       "   'precipitations_pca_2',\n",
       "   'tempartures_lag_1w_pca_1',\n",
       "   'tempartures_pca_1',\n",
       "   'soil_moisture_pca_2',\n",
       "   'soil_moisture_pca_3',\n",
       "   'soil_composition_pca_1',\n",
       "   'soil_composition_pca_2',\n",
       "   'soil_composition_pca_3',\n",
       "   'soil_composition_pca_4',\n",
       "   'soil_composition_pca_6',\n",
       "   'soil_composition_pca_7',\n",
       "   'soil_composition_pca_9',\n",
       "   'latitude',\n",
       "   'longitude',\n",
       "   'catchment',\n",
       "   'altitude',\n",
       "   'water_flow_lag_1w',\n",
       "   'water_flow_lag_2w',\n",
       "   'water_flow_ma_4w_lag_1w_gauss',\n",
       "   'month_cos',\n",
       "   'season_sin',\n",
       "   'hydro_cluster'],\n",
       "  'nb_cols': 24},\n",
       " 2: {'best_params_': {'preprocessor__kbest_select__k': np.int64(6),\n",
       "   'model__n_estimators': 190,\n",
       "   'model__min_samples_split': 105,\n",
       "   'model__min_samples_leaf': 80,\n",
       "   'model__max_features': 'log2',\n",
       "   'model__max_depth': 20,\n",
       "   'model__bootstrap': False},\n",
       "  'best_score_': np.float64(-3.0582488454553585),\n",
       "  'selected_features': ['soil_composition_pca_8',\n",
       "   'soil_composition_pca_9',\n",
       "   'catchment',\n",
       "   'water_flow_lag_1w',\n",
       "   'water_flow_lag_2w',\n",
       "   'water_flow_ma_4w_lag_1w_gauss'],\n",
       "  'nb_cols': 6}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"loc_clust_week4_config.json\", 'w') as json_file:\n",
    "    json.dump(loc_params, json_file, indent=4, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zzzz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mzzzz\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'zzzz' is not defined"
     ]
    }
   ],
   "source": [
    "zzzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    (\"selector\", SelectKBest(score_func=f_regression)),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"rm_wl\", # remove custom generated water_flow_lag 3w & 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"slct_ma\", # keep only specific mobile average 2w or/and 3w or/and 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"lag_slope\" # add an indicator that is calculated between water_flow_lag 1w and 2w \n",
    "]\n",
    "\n",
    "PCA_THRESHOLD = 0.98\n",
    "N_CLUSTER = 10\n",
    "SELECTED_MOBILE_AVERAGE = [\n",
    "    # \"water_flow_ma_4w_lag_1w\",\n",
    "    # \"water_flow_ma_3w_lag_1w\",\n",
    "    # \"water_flow_ma_2w_lag_1w\",\n",
    "    \"water_flow_ma_4w_lag_1w_gauss\",\n",
    "    # \"water_flow_ma_3w_lag_1w_gauss\",\n",
    "    # \"water_flow_ma_2w_lag_1w_gauss\"\n",
    "]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'selector__k': np.int64(27), 'model__n_estimators': 85, 'model__min_samples_split': 80, 'model__min_samples_leaf': 15, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -1.9577376316109347\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:\n",
      "['index', 'precipitations_lag_1w_pca_2', 'precipitations_pca_1', 'precipitations_pca_2', 'tempartures_lag_1w_pca_1', 'tempartures_pca_1', 'tempartures_pca_2', 'soil_moisture_pca_1', 'soil_moisture_pca_2', 'soil_moisture_pca_3', 'evaporation_lag_1w_pca_1', 'evaporation_pca_1', 'soil_composition_pca_1', 'soil_composition_pca_4', 'soil_composition_pca_6', 'soil_composition_pca_7', 'soil_composition_pca_9', 'latitude', 'longitude', 'catchment', 'altitude', 'water_flow_lag_1w', 'water_flow_lag_2w', 'water_flow_ma_4w_lag_1w_gauss', 'north_hemisphere', 'snow_index', 'month_cos']\n"
     ]
    }
   ],
   "source": [
    "best_selector = random_search.best_estimator_.named_steps[\"selector\"]\n",
    "\n",
    "selected_mask = best_selector.get_support()\n",
    "adjusted_data = cv_data.drop(columns=cols_to_drop)\n",
    "selected_features = adjusted_data.columns[selected_mask]\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(selected_features.tolist())\n",
    "with open(f\"{DATASET_SPEC}_selected_features_week{WEEK_TO_PREDICT}.json\", 'w') as json_file:\n",
    "    json.dump(selected_features.tolist(), json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    \"scl_feat\",\n",
    "    # \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"rm_wl\", # remove custom generated water_flow_lag 3w & 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"slct_ma\", # keep only specific mobile average 2w or/and 3w or/and 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"lag_slope\" # add an indicator that is calculated between water_flow_lag 1w and 2w \n",
    "]\n",
    "\n",
    "PCA_THRESHOLD = 0.98\n",
    "N_CLUSTER = 10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 35, 'model__min_samples_split': 9, 'model__min_samples_leaf': 25, 'model__max_features': None, 'model__max_depth': 50, 'model__bootstrap': True}\n",
      "Best Score: -1.8825202404996986\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    \"scl_feat\",\n",
    "    # \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"rm_wl\", # remove custom generated water_flow_lag 3w & 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"slct_ma\", # keep only specific mobile average 2w or/and 3w or/and 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"lag_slope\" # add an indicator that is calculated between water_flow_lag 1w and 2w \n",
    "]\n",
    "\n",
    "PCA_THRESHOLD = 0.98\n",
    "N_CLUSTER = 10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 100, 'model__min_samples_split': 50, 'model__min_samples_leaf': 25, 'model__max_features': None, 'model__max_depth': 70, 'model__bootstrap': True}\n",
      "Best Score: -1.9117332654881167\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"rm_wl\", # remove custom generated water_flow_lag 3w & 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"slct_ma\", # keep only specific mobile average 2w or/and 3w or/and 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"lag_slope\" # add an indicator that is calculated between water_flow_lag 1w and 2w \n",
    "]\n",
    "\n",
    "PCA_THRESHOLD = 0.98\n",
    "N_CLUSTER = 10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 35, 'model__min_samples_split': 11, 'model__min_samples_leaf': 23, 'model__max_features': None, 'model__max_depth': 40, 'model__bootstrap': True}\n",
      "Best Score: -1.8653183950465695\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"rm_wl\", # remove custom generated water_flow_lag 3w & 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"slct_ma\", # keep only specific mobile average 2w or/and 3w or/and 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
    "    \"lag_slope\" # add an indicator that is calculated between water_flow_lag 1w and 2w \n",
    "]\n",
    "\n",
    "PCA_THRESHOLD = 0.98\n",
    "N_CLUSTER = 10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 35, 'model__min_samples_split': 9, 'model__min_samples_leaf': 25, 'model__max_features': None, 'model__max_depth': 50, 'model__bootstrap': True}\n",
      "Best Score: -1.8821667848494648\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    # \"rm_wl\", # remove custom generated water_flow_lag 3w & 4w\n",
    "    # \"rm_ma\", # remove custom generated mobile average 2w & 3w & 4w\n",
    "    \"lag_slope\" # add an indicator that is calculated between water_flow_lag 1w and 2w \n",
    "]\n",
    "\n",
    "PCA_THRESHOLD = 0.98\n",
    "N_CLUSTER = 10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 160, 'model__min_samples_split': 66, 'model__min_samples_leaf': 42, 'model__max_features': None, 'model__max_depth': 25, 'model__bootstrap': True}\n",
      "Best Score: -1.9426425655553738\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "]\n",
    "\n",
    "PCA_THRESHOLD = 0.98\n",
    "N_CLUSTER = 5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 150, 'model__min_samples_split': 30, 'model__min_samples_leaf': 18, 'model__max_features': None, 'model__max_depth': 23, 'model__bootstrap': True}\n",
      "Best Score: -1.9362015137618989\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('snowindexer', SnowIndexComputeTransformer(temp_col_name=\"tempartures_pca_1\", rain_col_name=\"precipitations_pca_1\",)),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'snowindexer__temp_weight': 1.2, 'snowindexer__precip_weight': 0.15, 'snowindexer__altitude_weight': 0.9, 'model__n_estimators': 100, 'model__min_samples_split': 5, 'model__min_samples_leaf': 30, 'model__max_features': None, 'model__max_depth': 50, 'model__bootstrap': False}\n",
      "Best Score: -2.1058580480136575\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    # \"cyc_enc_date\",\n",
    "    # \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 100, 'model__min_samples_split': 10, 'model__min_samples_leaf': 40, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -1.9542643097106727\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    # \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    \"scl_feat\",\n",
    "    # \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 45, 'model__min_samples_split': 40, 'model__min_samples_leaf': 20, 'model__max_features': 0.7, 'model__max_depth': 30, 'model__bootstrap': False}\n",
      "Best Score: -1.9515434055944696\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    \"scl_feat\",\n",
    "    # \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 130, 'model__min_samples_split': 30, 'model__min_samples_leaf': 20, 'model__max_features': None, 'model__max_depth': 30, 'model__bootstrap': True}\n",
      "Best Score: -1.935871280814628\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"scl_catch\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 135, 'model__min_samples_split': 27, 'model__min_samples_leaf': 27, 'model__max_features': None, 'model__max_depth': 30, 'model__bootstrap': True}\n",
      "Best Score: -1.9339345588124381\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"clust_hydro\",\n",
    "    # \"scl_feat\",\n",
    "    \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"scl_catch\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 130, 'model__min_samples_split': 30, 'model__min_samples_leaf': 20, 'model__max_features': None, 'model__max_depth': 30, 'model__bootstrap': True}\n",
      "Best Score: -1.9395931333152727\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"scl_feat\",\n",
    "    # \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"scl_catch\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 23, 'model__min_samples_split': 15, 'model__min_samples_leaf': 21, 'model__max_features': 0.7, 'model__max_depth': 23, 'model__bootstrap': True}\n",
      "Best Score: -1.9359472748030164\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"scl_feat\",\n",
    "    # \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"scl_catch\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 20, 'model__min_samples_split': 10, 'model__min_samples_leaf': 20, 'model__max_features': 0.7, 'model__max_depth': 20, 'model__bootstrap': True}\n",
      "Best Score: -1.95292479637325\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"scl_feat\",\n",
    "    # \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"scl_catch\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 50, 'model__min_samples_split': 18, 'model__min_samples_leaf': 16, 'model__max_features': None, 'model__max_depth': 16, 'model__bootstrap': True}\n",
      "Best Score: -1.9292546040045893\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('snowindexer', SnowIndexComputeTransformer(temp_col_name=\"tempartures_pca_1\", rain_col_name=\"precipitations_pca_1\",)),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    # \"oh_enc_date\",\n",
    "    \"cyc_enc_date\",\n",
    "    \"clust_index\",\n",
    "    \"scl_feat\",\n",
    "    # \"scl_feat_wl\", # Scale all except waterflow lag\n",
    "    \"scl_catch\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 60, 'model__min_samples_split': 20, 'model__min_samples_leaf': 9, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -1.9860207069874143\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('snowindexer', SnowIndexComputeTransformer(temp_col_name=\"tempartures_pca_1\", rain_col_name=\"precipitations_pca_1\",)),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n",
    "\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    \"scl_catch\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'snowindexer__temp_weight': 1, 'snowindexer__precip_weight': 0.2, 'snowindexer__altitude_weight': 1, 'model__n_estimators': 52, 'model__min_samples_split': 19, 'model__min_samples_leaf': 13, 'model__max_features': None, 'model__max_depth': 25, 'model__bootstrap': True}\n",
      "Best Score: -2.0340065806334726\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    \"scl_catch\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 55, 'model__min_samples_split': 20, 'model__min_samples_leaf': 11, 'model__max_features': None, 'model__max_depth': 23, 'model__bootstrap': True}\n",
      "Best Score: -1.9197539988339372\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    \"scl_catch\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 60, 'model__min_samples_split': 20, 'model__min_samples_leaf': 9, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -1.9379500224596236\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    \"scl_catch\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 59, 'model__min_samples_split': 18, 'model__min_samples_leaf': 14, 'model__max_features': None, 'model__max_depth': 25, 'model__bootstrap': True}\n",
      "Best Score: -1.923900362267943\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    \"scl_catch\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 60, 'model__min_samples_split': 20, 'model__min_samples_leaf': 9, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -1.9528121007193104\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    # \"rm_st_id\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 57, 'model__min_samples_split': 20, 'model__min_samples_leaf': 14, 'model__max_features': None, 'model__max_depth': 22, 'model__bootstrap': True}\n",
      "Best Score: -2.067613593686774\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    # \"rm_st_id\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 55, 'model__min_samples_split': 22, 'model__min_samples_leaf': 16, 'model__max_features': None, 'model__max_depth': 25, 'model__bootstrap': True}\n",
      "Best Score: -1.9144132951754593\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    # \"rm_st_id\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 56, 'model__min_samples_split': 19, 'model__min_samples_leaf': 13, 'model__max_features': None, 'model__max_depth': 23, 'model__bootstrap': True}\n",
      "Best Score: -1.9230515071418086\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    # \"rm_st_id\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 57, 'model__min_samples_split': 20, 'model__min_samples_leaf': 11, 'model__max_features': None, 'model__max_depth': 17, 'model__bootstrap': True}\n",
      "Best Score: -1.9198867119123673\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    # \"rm_st_id\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 60, 'model__min_samples_split': 20, 'model__min_samples_leaf': 9, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -2.1232930047642933\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"rm_gnv_st\",\n",
    "    \"pca\",\n",
    "    # \"snow_index\",\n",
    "    \"oh_enc_date\",\n",
    "    \"scl_wtr_flows\",\n",
    "    # \"rm_st_id\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 60, 'model__min_samples_split': 20, 'model__min_samples_leaf': 9, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -2.1232930047642933\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for config : \n",
    "```python\n",
    "DATASET_TRANSFORMS = [\n",
    "    \"remove_geneve_station\",\n",
    "    \"full_pca\",\n",
    "    # \"snow_index\",\n",
    "    \"one_hot_encode_month_season\",\n",
    "    \"scale_train_waterflows\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 60, 'model__min_samples_split': 20, 'model__min_samples_leaf': 9, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -2.1232930047642933\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for qrf pca_and_sin_season_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 60, 'model__min_samples_split': 20, 'model__min_samples_leaf': 9, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -2.1232930047642933\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params for qrf full_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__n_estimators': 60, 'model__min_samples_split': 20, 'model__min_samples_leaf': 9, 'model__max_features': None, 'model__max_depth': 13, 'model__bootstrap': True}\n",
      "Best Score: -2.1232930047642933\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMING SOON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
