{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective: Final Prediction Computation\n",
        "\n",
        "This notebook generates the final model predictions and formats them for submission on Codabench.\n",
        "\n",
        "The evaluation dataset comprises data from 39 stations included in the training set and 13 stations exclusive to the evaluation set.\n",
        "\n",
        "<img src=\"../images/notebook-4.png\" alt=\"Experiment Diagram\" style=\"width:75%;\" style=\"text-align:center;\" />\n",
        "\n",
        "### 1. Imports\n",
        "\n",
        "Starts by importing the necessary libraries, configuring environment paths, and loading custom utility functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
        "\n",
        "from src.utils.model import load_models_auto\n",
        "from src.utils.analysis import create_predict_function, create_quantile_function\n",
        "from src.utils.model import load_models_auto\n",
        "from src.utils.model import (\n",
        "    XGBQRF_SimpleModel,\n",
        "    Ensemble,\n",
        ")\n",
        "\n",
        "# Some more magic so that the notebook will reload external python modules;\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defines constants :\n",
        "\n",
        "- _DATASET_DIR_ must be the directory where you unzip the _zenodo_ dataset.\n",
        "- _EVAL_DIR_ will be used to store inference / evaluation data it must be the same as the one defined in _01 Training > 01 - Modelisation_\n",
        "- _FINAL_MODEL_ will be used to store inference / evaluation data\n",
        "\n",
        "FINAL_MODEL describe the model that will be loaded if you use auto-loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PREDS_DIR: ../../../data/evaluation/rm_gnv_st_pca_snow_index_snow_index_lag_cyc_enc_date_clust_index_clust_index_oh_clust_hydro_clust_hydro_oh_slct_ma_lag_slope_pct_0.98_geocl_5_hydcl_5/ensemble/\n"
          ]
        }
      ],
      "source": [
        "ALPHA = 0.1\n",
        "NUMBER_OF_WEEK = 4\n",
        "USE_AUTO_SCAN = True  # Toggle this to switch between the loading of the last model of the manual load of a specific model\n",
        "FINAL_MODEL = \"ensemble\"\n",
        "\n",
        "# Should be the same as in the training and feature engineering notebook\n",
        "# This list should be the same as the one used in the previous notebook as it will be used to fetch the data\n",
        "DATASET_TRANSFORMS = [\n",
        "    \"rm_gnv_st\",  # Remove the geneva station (which is an outlier in the dataset) - Recommended\n",
        "    \"pca\",  # Apply PCA to some of the static features (soil composition etc...) - Recommended\n",
        "    \"snow_index\",  # Compute the snow index - probability of snow (which is a feature that's estimated from other features in the dataset) - Recommended\n",
        "    \"snow_index_lag\",  # Add lagged features of the snow index - Should be disabled if snow_index is not used\n",
        "    # \"oh_enc_date\", # One hot encoding of the date - Not recommended use \"cyc_enc_date\" instead\n",
        "    \"cyc_enc_date\",  # Cyclic encoding of the date - Recommended (https://developer.nvidia.com/blog/three-approaches-to-encoding-time-information-as-features-for-ml-models/)\n",
        "    \"clust_index\",  # Clustering based on the location of the stations\n",
        "    \"clust_index_oh\",  # One hot encoding of the cluster index - Recommended\n",
        "    \"clust_hydro\",  # Clustering based on the hydro features\n",
        "    \"clust_hydro_oh\",  # One hot encoding of the hydro cluster index - Recommended\n",
        "    # \"scl_feat\", # Scaling to all the features - Not recommended for boosting models (Recommended for NN)\n",
        "    # \"scl_feat_wl\",  # Scale all except waterflow lag\n",
        "    # \"rm_wl\",  # remove custom generated water_flow_lag 3w & 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
        "    \"slct_ma\",  # keep only specific mobile average 2w or/and 3w or/and 4w ---> Need USE_CUSTOM_PREPROCESS = True\n",
        "    \"lag_slope\",  # add an indicator that is calculated between water_flow_lag 1w and 2w\n",
        "    # \"keep_precipitation_and_evaporation\",  # keep precipitation and evaporation features without pca\n",
        "    # \"target_precipitation\", # Lagged precipitation feature - experimental should be disabled\n",
        "]\n",
        "\n",
        "PCA_THRESHOLD = 0.98\n",
        "N_CLUSTER = 5\n",
        "\n",
        "DATASET_SPEC = \"_\".join(DATASET_TRANSFORMS)\n",
        "# DATASET_SPEC = \"custom_dataset_4\"\n",
        "\n",
        "if \"pca\" in DATASET_TRANSFORMS:\n",
        "    DATASET_SPEC += f\"_pct_{PCA_THRESHOLD}\"\n",
        "\n",
        "if \"clust_index\" in DATASET_TRANSFORMS:\n",
        "    DATASET_SPEC += f\"_geocl_{N_CLUSTER}\"\n",
        "\n",
        "if \"clust_hydro\" in DATASET_TRANSFORMS:\n",
        "    DATASET_SPEC += f\"_hydcl_{N_CLUSTER}\"\n",
        "\n",
        "# DATASET_SPEC = \"custom_dataset_4\"\n",
        "\n",
        "ADJUSTED_BONDS = False\n",
        "\n",
        "EVAL_DIR = \"../../../data/evaluation/\"\n",
        "EVAL_DIR_MINI = \"../../../data/evaluation_mini/\"\n",
        "MODEL_DIR = f\"../../../models/{DATASET_SPEC}/\"\n",
        "\n",
        "PREDS_DIR = f\"{EVAL_DIR}{DATASET_SPEC}/{FINAL_MODEL}/\"\n",
        "COMPUTE_MINICHALLENGE = True\n",
        "\n",
        "USE_ONLY_BEST_FEATURES = False\n",
        "BEST_FEATURES = [\n",
        "    \"precipitations_lag_1w_pca_2\",\n",
        "    \"precipitations_pca_1\",\n",
        "    \"precipitations_pca_2\",\n",
        "    \"tempartures_lag_1w_pca_1\",\n",
        "    \"tempartures_pca_1\",\n",
        "    \"tempartures_pca_2\",\n",
        "    \"soil_moisture_pca_1\",\n",
        "    \"soil_moisture_pca_2\",\n",
        "    \"soil_moisture_pca_3\",\n",
        "    \"evaporation_lag_1w_pca_1\",\n",
        "    \"evaporation_pca_1\",\n",
        "    \"soil_composition_pca_1\",\n",
        "    \"soil_composition_pca_4\",\n",
        "    \"soil_composition_pca_6\",\n",
        "    \"soil_composition_pca_7\",\n",
        "    \"soil_composition_pca_9\",\n",
        "    \"latitude\",\n",
        "    \"longitude\",\n",
        "    \"catchment\",\n",
        "    \"altitude\",\n",
        "    \"water_flow_lag_1w\",\n",
        "    \"water_flow_lag_2w\",\n",
        "    \"water_flow_ma_4w_lag_1w_gauss\",\n",
        "    \"north_hemisphere\",\n",
        "    \"snow_index\",\n",
        "    \"month_cos\",\n",
        "]\n",
        "\n",
        "print(f\"PREDS_DIR: {PREDS_DIR}\")\n",
        "os.makedirs(PREDS_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_COLUMNS = [f\"water_flow_week{i+1}\" for i in range(NUMBER_OF_WEEK)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data and models Loading\n",
        "\n",
        "Loading of the inference dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "inference_data = pd.read_csv(f\"{EVAL_DIR}dataset_{DATASET_SPEC}.csv\")\n",
        "inference_data = inference_data.set_index(\"ObsDate\")\n",
        "\n",
        "if COMPUTE_MINICHALLENGE:\n",
        "    inference_data_mini = pd.read_csv(f\"{EVAL_DIR_MINI}dataset_{DATASET_SPEC}.csv\")\n",
        "    inference_data_mini = inference_data_mini.set_index(\"ObsDate\")\n",
        "    inference_data = pd.concat([inference_data, inference_data_mini], axis=0)\n",
        "    inference_data.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in inference_data.columns:\n",
        "    if inference_data[col].dropna().apply(lambda x: isinstance(x, bool)).any():\n",
        "        inference_data[col] = inference_data[col].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading of the final models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model for week 0: ensemble_2025-05-09_11-42-44_week_0.pkl\n",
            "Loading model for week 1: ensemble_2025-05-09_11-43-45_week_1.pkl\n",
            "Loading model for week 2: ensemble_2025-05-09_11-48-40_week_2.pkl\n",
            "Loading model for week 3: ensemble_2025-05-09_11-49-05_week_3.pkl\n"
          ]
        }
      ],
      "source": [
        "# Load models based on conditions\n",
        "final_models = []\n",
        "if FINAL_MODEL == \"mapie\":\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"mapie_quantile\", f\"{MODEL_DIR}final/\")\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(\n",
        "                f\"{MODEL_DIR}final/mapie_quantile_2025-01-17_15-15-04_week0.pkl\"\n",
        "            )\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(\n",
        "                f\"{MODEL_DIR}final/mapie_quantile_2025-01-17_15-15-11_week1.pkl\"\n",
        "            )\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(\n",
        "                f\"{MODEL_DIR}final/mapie_quantile_2025-01-17_15-15-17_week2.pkl\"\n",
        "            )\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(\n",
        "                f\"{MODEL_DIR}final/mapie_quantile_2025-01-17_15-15-17_week3.pkl\"\n",
        "            )\n",
        "        )\n",
        "elif FINAL_MODEL == \"qrf\":\n",
        "\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"qrf_quantile\", f\"{MODEL_DIR}final/\")\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-04_week0.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-11_week1.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week2.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week3.pkl\")\n",
        "        )\n",
        "elif FINAL_MODEL == \"gpr\":\n",
        "    selected_kernel = [\n",
        "        \"rbf\",\n",
        "        # \"\",\n",
        "        # \"\",\n",
        "    ]\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\n",
        "            f\"gpr_quantile_{\"\".join(selected_kernel)}\", f\"{MODEL_DIR}final/\"\n",
        "        )\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-04_week0.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-11_week1.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week2.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week3.pkl\")\n",
        "        )\n",
        "\n",
        "elif FINAL_MODEL == \"gbr\":\n",
        "\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"gbr_quantile\", f\"{MODEL_DIR}final/\")\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-04_week0.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-11_week1.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week2.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week3.pkl\")\n",
        "        )\n",
        "elif FINAL_MODEL == \"qrf_voting\":\n",
        "\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"qrf_voting_quantile\", f\"{MODEL_DIR}final/\")\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-04_week0.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-11_week1.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week2.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week3.pkl\")\n",
        "        )\n",
        "elif FINAL_MODEL == \"qrf_bagging\":\n",
        "\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"qrf_bagging_quantile\", f\"{MODEL_DIR}final/\")\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-04_week0.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-11_week1.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week2.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week3.pkl\")\n",
        "        )\n",
        "elif FINAL_MODEL == \"lgbm\":\n",
        "\n",
        "    if USE_AUTO_SCAN:\n",
        "        models_low = load_models_auto(\"lgbm_quantile_q0.05\", f\"{MODEL_DIR}final/\")\n",
        "        models_med = load_models_auto(\"lgbm_quantile_q0.5\", f\"{MODEL_DIR}final/\")\n",
        "        models_high = load_models_auto(\"lgbm_quantile_q0.95\", f\"{MODEL_DIR}final/\")\n",
        "        final_models = [[] for _ in range(NUMBER_OF_WEEK)]\n",
        "        final_models[0] = [models_low[0], models_med[0], models_high[0]]\n",
        "        final_models[1] = [models_low[1], models_med[1], models_high[1]]\n",
        "        final_models[2] = [models_low[2], models_med[2], models_high[2]]\n",
        "        final_models[3] = [models_low[3], models_med[3], models_high[3]]\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-04_week0.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-11_week1.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week2.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/qrf_quantile_2025-01-17_15-15-17_week3.pkl\")\n",
        "        )\n",
        "elif FINAL_MODEL == \"ebm_ensemble\":\n",
        "    print(\"Loading EBM Ensemble\")\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"ebm_ensemble\", f\"{MODEL_DIR}final/\")\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/ebm_ensemble_2025-01-17_15-15-04_week0.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/ebm_ensemble_2025-01-17_15-15-11_week1.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/ebm_ensemble_2025-01-17_15-15-17_week2.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/ebm_ensemble_2025-01-17_15-15-17_week3.pkl\")\n",
        "        )\n",
        "elif FINAL_MODEL == \"deep_ensemble\":\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"deep_ensemble\", f\"{MODEL_DIR}final/\")\n",
        "    else:\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/deep_ensemble_2025-01-17_15-15-04_week0.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/deep_ensemble_2025-01-17_15-15-11_week1.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/deep_ensemble_2025-01-17_15-15-17_week2.pkl\")\n",
        "        )\n",
        "        final_models.append(\n",
        "            joblib.load(f\"{MODEL_DIR}final/deep_ensemble_2025-01-17_15-15-17_week3.pkl\")\n",
        "        )\n",
        "elif FINAL_MODEL == \"xgb\":\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"xgb\", f\"{MODEL_DIR}final/\")\n",
        "elif FINAL_MODEL == \"xgb_qrf\":\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"xgb_qrf\", f\"{MODEL_DIR}final/\")\n",
        "elif FINAL_MODEL == \"xgb_qrf_simple\":\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"xgb_qrf_simple\", f\"{MODEL_DIR}final/\")\n",
        "elif FINAL_MODEL == \"ensemble\":\n",
        "    if USE_AUTO_SCAN:\n",
        "        final_models = load_models_auto(\"ensemble\", f\"{MODEL_DIR}final/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Predictions computation\n",
        "\n",
        "Evaluation data include a spatio-temporal split and a temporal only split.\n",
        "\n",
        "<img src=\"../images/eval.png\" alt=\"Experiment Diagram\" style=\"width:50%;\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ensemble\n",
            "model : ensemble\n",
            "ensemble\n",
            "model : ensemble\n",
            "ensemble\n",
            "model : ensemble\n",
            "ensemble\n",
            "model : ensemble\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = inference_data[[\"station_code\"]].copy()\n",
        "y_pred_test_quantile = {}\n",
        "y_pred_test = {}\n",
        "X_test = inference_data.drop(columns=[\"station_code\"])\n",
        "\n",
        "if len(set(TARGET_COLUMNS).intersection(list(X_test.columns))) > 0:\n",
        "    X_test = X_test.drop(columns=TARGET_COLUMNS)\n",
        "\n",
        "if USE_ONLY_BEST_FEATURES:\n",
        "    X_test = X_test[BEST_FEATURES]\n",
        "\n",
        "\n",
        "if FINAL_MODEL == \"chained_qrf\":\n",
        "    # y_pred_test = final_models.predict(X_test, quantiles=[0.04, 0.5, 0.96])\n",
        "    y_pred_test = final_models.predict(X_test)\n",
        "\n",
        "    for i in range(NUMBER_OF_WEEK):\n",
        "        predictions[f\"week_{i}_pred\"] = y_pred_test[i][:, 1]\n",
        "        predictions[f\"week_{i}_sup\"] = y_pred_test[i][:, 2]\n",
        "        predictions[f\"week_{i}_inf\"] = y_pred_test[i][:, 0]\n",
        "else:\n",
        "    for i in range(NUMBER_OF_WEEK):\n",
        "\n",
        "        if FINAL_MODEL == \"qrf\":\n",
        "            # reorder the columns\n",
        "            X_test = X_test[final_models[0].feature_names_in_]\n",
        "\n",
        "        # if FINAL_MODEL == \"xgb\":\n",
        "        #     X_test = (\n",
        "        #         X_test.drop(columns=[\"north_hemisphere\"])\n",
        "        #         if \"north_hemisphere\" in X_test.columns\n",
        "        #         else X_test\n",
        "        #     )\n",
        "        print(FINAL_MODEL)\n",
        "        predict_adjusted = create_predict_function(\n",
        "            final_models, i, FINAL_MODEL, prev_week_models=final_models[:i]\n",
        "        )\n",
        "        quantile_adjusted = create_quantile_function(\n",
        "            final_models, i, FINAL_MODEL, ALPHA, prev_week_models=final_models[:i]\n",
        "        )\n",
        "\n",
        "        y_pred_test[i] = predict_adjusted(X_test)\n",
        "        y_pred_test_quantile[i] = quantile_adjusted(X_test)\n",
        "\n",
        "        if FINAL_MODEL == \"lgbm\":\n",
        "            y_pred_test_quantile[i][y_pred_test_quantile[i] < 0] = 0\n",
        "            y_pred_test[i][y_pred_test[i] < 0] = 0\n",
        "\n",
        "        if FINAL_MODEL == \"xgb\":\n",
        "            y_pred_test_quantile[i][:, 0] *= 0.98\n",
        "            y_pred_test_quantile[i][:, 1] *= 1.02\n",
        "\n",
        "    for i in range(NUMBER_OF_WEEK):\n",
        "        predictions[f\"week_{i}_pred\"] = y_pred_test[i]\n",
        "        predictions[f\"week_{i}_sup\"] = y_pred_test_quantile[i][:, 1]\n",
        "        predictions[f\"week_{i}_inf\"] = y_pred_test_quantile[i][:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Saving of the predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving of the predictions as a csv file\n",
        "\n",
        "> The file must be named `predictions.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['precipitations_lag_1w_pca_1', 'precipitations_lag_1w_pca_2',\n",
              "       'precipitations_pca_1', 'precipitations_pca_2',\n",
              "       'tempartures_lag_1w_pca_1', 'tempartures_pca_1', 'soil_moisture_pca_1',\n",
              "       'soil_moisture_pca_2', 'soil_moisture_pca_3',\n",
              "       'evaporation_lag_1w_pca_1', 'evaporation_pca_1',\n",
              "       'soil_composition_pca_1', 'soil_composition_pca_2',\n",
              "       'soil_composition_pca_3', 'soil_composition_pca_4',\n",
              "       'soil_composition_pca_5', 'soil_composition_pca_6',\n",
              "       'soil_composition_pca_7', 'latitude', 'longitude', 'catchment',\n",
              "       'altitude', 'water_flow_lag_1w', 'water_flow_lag_2w',\n",
              "       'water_flow_lag_3w', 'water_flow_lag_4w',\n",
              "       'water_flow_ma_4w_lag_1w_gauss', 'north_hemisphere', 'snow_index',\n",
              "       'snow_index_lag1', 'snow_index_lag2', 'snow_index_lag3',\n",
              "       'snow_index_lag4', 'month_sin', 'month_cos', 'season_sin', 'season_cos',\n",
              "       'region_cluster_0', 'region_cluster_1', 'region_cluster_2',\n",
              "       'region_cluster_3', 'region_cluster_4', 'hydro_cluster_0',\n",
              "       'hydro_cluster_1', 'hydro_cluster_2', 'hydro_cluster_3',\n",
              "       'hydro_cluster_4', 'water_flow_evolve_slope'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>week_1_inf</th>\n",
              "      <th>week_1_pred</th>\n",
              "      <th>week_1_sup</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ObsDate</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2004-02-01</th>\n",
              "      <td>18.763600</td>\n",
              "      <td>36.320000</td>\n",
              "      <td>79.728857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004-04-25</th>\n",
              "      <td>9.846786</td>\n",
              "      <td>25.697143</td>\n",
              "      <td>64.937715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004-07-18</th>\n",
              "      <td>7.261286</td>\n",
              "      <td>13.775143</td>\n",
              "      <td>34.818428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004-10-10</th>\n",
              "      <td>2.039543</td>\n",
              "      <td>3.850857</td>\n",
              "      <td>9.899971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-01-02</th>\n",
              "      <td>9.007552</td>\n",
              "      <td>16.447140</td>\n",
              "      <td>46.857142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-04-20</th>\n",
              "      <td>600.339415</td>\n",
              "      <td>1327.004761</td>\n",
              "      <td>2569.810062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-07-13</th>\n",
              "      <td>245.386389</td>\n",
              "      <td>344.368604</td>\n",
              "      <td>515.568689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-10-05</th>\n",
              "      <td>101.416434</td>\n",
              "      <td>211.795197</td>\n",
              "      <td>424.911617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-12-28</th>\n",
              "      <td>943.059442</td>\n",
              "      <td>1644.410449</td>\n",
              "      <td>2919.547060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-22</th>\n",
              "      <td>311.828394</td>\n",
              "      <td>940.789734</td>\n",
              "      <td>1751.645610</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1390 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            week_1_inf  week_1_pred   week_1_sup\n",
              "ObsDate                                         \n",
              "2004-02-01   18.763600    36.320000    79.728857\n",
              "2004-04-25    9.846786    25.697143    64.937715\n",
              "2004-07-18    7.261286    13.775143    34.818428\n",
              "2004-10-10    2.039543     3.850857     9.899971\n",
              "2005-01-02    9.007552    16.447140    46.857142\n",
              "...                ...          ...          ...\n",
              "2014-04-20  600.339415  1327.004761  2569.810062\n",
              "2014-07-13  245.386389   344.368604   515.568689\n",
              "2014-10-05  101.416434   211.795197   424.911617\n",
              "2014-12-28  943.059442  1644.410449  2919.547060\n",
              "2015-03-22  311.828394   940.789734  1751.645610\n",
              "\n",
              "[1390 rows x 3 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[[\"week_1_inf\", \"week_1_pred\", \"week_1_sup\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.0215571426772009)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = predictions[\"week_1_pred\"] - predictions[\"week_1_inf\"]\n",
        "test.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the predictions to a csv file\n",
        "predictions[\"ObsDate\"] = X_test.index\n",
        "predictions.to_csv(f\"{PREDS_DIR}predictions.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compression of the submission file.\n",
        "\n",
        "> The file need to be compress for Codabench.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a ZIP file containing predictions.csv\n",
        "with zipfile.ZipFile(f\"{PREDS_DIR}predictions.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipf.write(f\"{PREDS_DIR}predictions.csv\", \"predictions.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You are ready to submit go to codabench and submit the zip file that have been generated in My Submissions > Phase 1.\n",
        "\n",
        "You don't have to use this notebook to submit but the file file format must includes the following columns:\n",
        "\n",
        "- station_code: Identification code of the station.\n",
        "- ObsDate: Date of the prediction.\n",
        "- for every week of prediction i from 0 to 3 :\n",
        "  - week_i_pred\n",
        "  - week_i_inf\n",
        "  - week_i_sup\n",
        "\n",
        "Save the dataset as a CSV file named predictions.csv.\n",
        "\n",
        "> The file must be named predictions.csv, but the .zip file can have any name.\n",
        "\n",
        "Compress the CSV file into a .zip archive.\n",
        "\n",
        "> You cannot submit an uncompressed file. Ensure that the software you use does not create a subfolder inside the archive.\n",
        "\n",
        "Submit your file in [Codabench](https://www.codabench.org/competitions/4335):\n",
        "\n",
        "> My Submissions > Phase 1 (keep all the tasks selected):\n",
        "\n",
        "<img src=\"../images/submissions.png\" alt=\"Experiment Diagram\" style=\"width:75%;\" style=\"text-align:center;\" />\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
