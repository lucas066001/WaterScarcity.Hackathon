{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Hyper Parameter Optimisation of *QRF* model\n",
    "\n",
    "This notebook propose different methods of hyper parameter optimisation based on X-Validation :\n",
    "* Random Search\n",
    "* Genetic algorithm [Not yet included]\n",
    "\n",
    "# 1. Data Import and Setup\n",
    "\n",
    "Imports necessary libraries, sets up environment paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Third-party imports\n",
    "from functools import partial\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Append project root to sys.path for local imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', '..')))\n",
    "\n",
    "# Local application imports\n",
    "from src.utils.model import split_dataset, get_station_stats, custom_log_likelihood\n",
    "from src.utils.SpatioTemporalSplit import SpatioTemporalSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines constants :\n",
    "* INPUT_DIR must be the same as the one defined in *00 Preprocessing/Feature Engineering*.\n",
    "* MODEL_DIR is the directory where the exploration models will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../../data/input/\"\n",
    "MODEL_DIR = \"../../../../models/exploration/\"\n",
    "\n",
    "SEED = 42 \n",
    "ALPHA = 0.1\n",
    "WEEK_TO_PREDICT=1 \n",
    "\n",
    "# columns to dropÂ : target at different horizon, station_code, and features removed from Feature Selection\n",
    "TO_DROP = [\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading\n",
    "Load in the baseline datasets, create the directory to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "ds_train = pd.read_csv(f\"{INPUT_DIR}dataset_baseline.csv\")\n",
    "train_data = ds_train.copy()\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data = train_data.loc[:, ~train_data.columns.duplicated()]\n",
    "ds_train = ds_train.set_index(\"ObsDate\")\n",
    "y_train = train_data[f\"water_flow_week{WEEK_TO_PREDICT}\"]\n",
    "cv_data = train_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model preparation\n",
    "\n",
    "Compute station statistics (usefull for scalling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_stats = get_station_stats(\n",
    "    y_train.to_numpy(),\n",
    "    train_data[\"station_code\"].to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom Pipeline to keep track of the station code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = TO_DROP.copy()\n",
    "cols_to_drop += [\"ObsDate\"]\n",
    "predictor_cols = [col for col in cv_data.columns if col not in cols_to_drop]\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('select', 'passthrough', predictor_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "qrf_week1 = RandomForestQuantileRegressor(n_estimators=10, max_depth=10, min_samples_leaf=10)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the log likelihood scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = partial(custom_log_likelihood,\n",
    "                 cv_data=cv_data,\n",
    "                 station_stats=station_stats,\n",
    "                 alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the SpatioTemporal Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = SpatioTemporalSplit(\n",
    "    n_splits=10,\n",
    "    date_col='ObsDate',\n",
    "    station_col='station_code',\n",
    "    temporal_frac=0.75,\n",
    "    spatial_frac=0.75,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyper parameter tuning\n",
    "\n",
    "Define the hyperparameter distributions for random search, take care the parameters presented here are choosen so that the search is fast you need to explore wider parameters range.\n",
    "\n",
    "#### a. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_distributions = {\n",
    "    'model__n_estimators': [5, 10, 15],\n",
    "    'model__max_depth': [5, 6, 7],\n",
    "    'model__min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# 9. Set up RandomizedSearchCV.\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,            # Number of parameter settings sampled\n",
    "    scoring=scorer,       # Use our custom scorer\n",
    "    cv=cv,                # Our custom spatio-temporal splitter\n",
    "    random_state=42,\n",
    "    n_jobs=-1             # Use all available cores\n",
    ")\n",
    "\n",
    "random_search.fit(cv_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMING SOON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
